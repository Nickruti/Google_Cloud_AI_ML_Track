{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Early Reinforcement Learning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure the right version of Tensorflow is installed.\n!pip install tensorflow==2.1 --user","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tensorflow==2.1\n  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n\u001b[K     |████████████████████████████████| 421.8 MB 29 kB/s s eta 0:00:01  |                                | 368 kB 4.6 MB/s eta 0:01:32     |█▏                              | 15.5 MB 4.6 MB/s eta 0:01:29     |████▏                           | 55.4 MB 42.5 MB/s eta 0:00:09     |█████▌                          | 71.9 MB 42.5 MB/s eta 0:00:09     |████████▏                       | 108.0 MB 40.8 MB/s eta 0:00:08     |████████████████▍               | 215.7 MB 16.2 MB/s eta 0:00:13     |████████████████████▍           | 269.4 MB 43.6 MB/s eta 0:00:04     |█████████████████████████▉      | 340.8 MB 33.1 MB/s eta 0:00:03     |██████████████████████████      | 342.8 MB 33.1 MB/s eta 0:00:03     |███████████████████████████▊    | 366.0 MB 43.7 MB/s eta 0:00:02\n\u001b[?25hCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nCollecting keras-preprocessing>=1.1.0\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting wrapt>=1.11.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\nCollecting google-pasta>=0.1.6\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[K     |████████████████████████████████| 57 kB 6.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting termcolor>=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nRequirement already satisfied: six>=1.12.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow==2.1) (1.15.0)\nCollecting protobuf>=3.8.0\n  Downloading protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n\u001b[K     |████████████████████████████████| 1.3 MB 23.8 MB/s eta 0:00:01\n\u001b[?25hCollecting gast==0.2.2\n  Downloading gast-0.2.2.tar.gz (10 kB)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow==2.1) (1.19.1)\nCollecting absl-py>=0.7.0\n  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n\u001b[K     |████████████████████████████████| 127 kB 31.3 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n\u001b[K     |████████████████████████████████| 448 kB 20.4 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 24.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorflow==2.1) (0.34.2)\nCollecting keras-applications>=1.0.8\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 8.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting grpcio>=1.8.6\n  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 21.4 MB/s eta 0:00:01     |███▌                            | 419 kB 21.4 MB/s eta 0:00:01\n\u001b[?25hCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 5.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting scipy==1.4.1; python_version >= \"3\"\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n\u001b[K     |████████████████████████████████| 26.1 MB 19.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.1) (49.2.0.post20200712)\nRequirement already satisfied: requests<3,>=2.21.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.24.0)\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.1-py3-none-any.whl (95 kB)\n\u001b[K     |████████████████████████████████| 95 kB 7.2 MB/s  eta 0:00:01\n\u001b[?25hCollecting google-auth<2,>=1.6.3\n  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n\u001b[K     |████████████████████████████████| 114 kB 23.2 MB/s eta 0:00:01\n\u001b[?25hCollecting werkzeug>=0.11.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n\u001b[K     |████████████████████████████████| 298 kB 23.9 MB/s eta 0:00:01\n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\nCollecting h5py\n  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n\u001b[K     |████████████████████████████████| 2.9 MB 27.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.25.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.7.0)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n\u001b[K     |████████████████████████████████| 155 kB 23.4 MB/s eta 0:00:01\n\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n\u001b[K     |████████████████████████████████| 47 kB 284 kB/s  eta 0:00:01\n\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\nCollecting pyasn1<0.5.0,>=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n\u001b[K     |████████████████████████████████| 77 kB 8.2 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.1)\nBuilding wheels for collected packages: wrapt, termcolor, gast\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69745 sha256=bd8a0c9b3145ae6998d26d0cfef966c5c51bcf9f1007a4c466507ddb8c0486e3\n  Stored in directory: /home/jovyan/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=d956f369884bdb698f27a27db2da2ad1c98d22c0340f557bd0aff0696e126d68\n  Stored in directory: /home/jovyan/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n  Building wheel for gast (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=94c6c74a648066cd937053990cfd49573b6baa504d83e10ce909a9e3439a160b\n  Stored in directory: /home/jovyan/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\nSuccessfully built wrapt termcolor gast\nInstalling collected packages: astor, keras-preprocessing, wrapt, google-pasta, termcolor, protobuf, gast, absl-py, tensorflow-estimator, grpcio, markdown, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, werkzeug, requests-oauthlib, google-auth-oauthlib, tensorboard, h5py, keras-applications, opt-einsum, scipy, tensorflow\nSuccessfully installed absl-py-0.10.0 astor-0.8.1 cachetools-4.1.1 gast-0.2.2 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.1 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gym==0.12.5 --user","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting gym==0.12.5\n  Downloading gym-0.12.5.tar.gz (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 6.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy in /home/jovyan/.local/lib/python3.6/site-packages (from gym==0.12.5) (1.4.1)\nRequirement already satisfied: numpy>=1.10.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from gym==0.12.5) (1.19.1)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.6/site-packages (from gym==0.12.5) (1.15.0)\nCollecting pyglet>=1.2.0\n  Downloading pyglet-1.5.7-py3-none-any.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 28.7 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: gym\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.12.5-py3-none-any.whl size=1613801 sha256=2a20a651294b02df3d27a58411235860c40a80eba36cb2b17afbc8db787d3e7a\n  Stored in directory: /home/jovyan/.cache/pip/wheels/98/2c/77/ae8eecab581ed913086cc59b17f69c54eaca71b6ba848fd2e9\nSuccessfully built gym\nInstalling collected packages: pyglet, gym\nSuccessfully installed gym-0.12.5 pyglet-1.5.7\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''here are four methods from Gym that are going to be useful to us in order to save the gumdrop.\n\nmake allows us to build the environment or game that we can pass actions to\nreset will reset an environment to it's starting configuration and return the state of the player\nrender displays the environment for human eyes\nstep takes an action and returns the player's next state.\nLet's make, reset, and render the game. The output is an ANSI string with the following characters:\n\nS for starting point\nF for frozen\nH for hole\nG for goal\nA red square indicates the current position\nNote: Restart the kernel if the above libraries needed to be installed'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport numpy as np\nimport random\nenv = gym.make('FrozenLake-v0', is_slippery=False)\nstate = env.reset()\nenv.render()","execution_count":1,"outputs":[{"output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(state)","execution_count":2,"outputs":[{"output_type":"stream","text":"0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#a simple print function to let us know whether it's game won, game over, or game on.\ndef print_state(state, done):\n    statement = \"Still Alive!\"\n    if done:\n        statement = \"Cocoa Time!\" if state == 15 else \"Game Over!\" \n    print(state, \"-\", statement)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0 left\n#1 down\n#2 right\n#3 up\n\n# Uncomment to reset the game\n#env.reset()\naction = 2  # Change me, please!\nstate, _, done, _ = env.step(action)\nenv.render()\nprint_state(state, done)","execution_count":20,"outputs":[{"output_type":"stream","text":"  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n15 - Cocoa Time!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Were you able to reach the hot chocolate? If so, great job!\nThere are multiple paths through the maze. One solution is [1, 1, 2, 2, 1, 2].\nLet's loop through our actions in order to get used to interacting with the environment programmatically.'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_game(actions):\n    state = env.reset()\n    step = 0\n    done = False\n\n    while not done and step < len(actions):\n        action = actions[step]\n        state, _, done, _ = env.step(action)\n        env.render()\n        step += 1\n        print_state(state, done)\n        \nactions = [1, 1, 2, 2, 1, 2]  # Replace with your favorite path.\nplay_game(actions)","execution_count":21,"outputs":[{"output_type":"stream","text":"  (Down)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFF\u001b[41mF\u001b[0mH\nHFFG\n10 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n15 - Cocoa Time!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nice, so we know how to get through the maze, but how do we teach that to the gumdrop? \n#It's just some bytes in an android phone. It doesn't have our human insight.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VALUE ITERATION\n#Let's program it out and see it in action! We'll set up an array representing the lake with -1 as the holes, and 1 as the goal. \n#Then, we'll set up an array of zeros to start our iteration.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAKE = np.array([[0,  0,  0,  0],\n                 [0, -1,  0, -1],\n                 [0,  0,  0, -1],\n                 [-1, 0,  0,  1]])\nLAKE_WIDTH = len(LAKE[0])\nLAKE_HEIGHT = len(LAKE)\n\nDISCOUNT = .9  # Change me to be a value between 0 and 1.\ncurrent_values = np.zeros_like(LAKE)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThe Gym environment class has a handy property for finding the number of states in an environment called observation_space.\nIn our case, there a 16 integer states, so it will label it as \"Discrete\". \nSimilarly, action_space will tell us how many actions are available to the agent.\n\nLet's take advantage of these to make our code portable between different lakes sizes.'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"env.observation_space -\", env.observation_space)\nprint(\"env.observation_space.n -\", env.observation_space.n)\nprint(\"env.action_space -\", env.action_space)\nprint(\"env.action_space.n -\", env.action_space.n)\n\nSTATE_SPACE = env.observation_space.n\nACTION_SPACE = env.action_space.n\nSTATE_RANGE = range(STATE_SPACE)\nACTION_RANGE = range(ACTION_SPACE)","execution_count":23,"outputs":[{"output_type":"stream","text":"env.observation_space - Discrete(16)\nenv.observation_space.n - 16\nenv.action_space - Discrete(4)\nenv.action_space.n - 4\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_neighbor_value(state_x, state_y, values, action):\n    \"\"\"Returns the value of a state's neighbor.\n    \n    Args:\n        state_x (int): The state's horizontal position, 0 is the lake's left.\n        state_y (int): The state's vertical position, 0 is the lake's top.\n        values (float array): The current iteration's state values.\n        policy (int): Which action to check the value for.\n        \n    Returns:\n        The corresponding action's value.\n    \"\"\"\n    left = [state_y, state_x-1]\n    down = [state_y+1, state_x]\n    right = [state_y, state_x+1]\n    up = [state_y-1, state_x]\n    actions = [left, down, right, up]\n\n    direction = actions[action]\n    check_x = direction[1]\n    check_y = direction[0]\n        \n    is_boulder = check_y < 0 or check_y >= LAKE_HEIGHT \\\n        or check_x < 0 or check_x >= LAKE_WIDTH\n    \n    value = values[state_y, state_x]\n    if not is_boulder:\n        value = values[check_y, check_x]\n        \n    return value","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nBut this doesn't find the best action, and the gumdrop is going to need that if it wants to greedily get off the lake.\nThe `get_max_neighbor` function we've defined below takes a number corresponding to a cell as `state_number` \nand the same value mapping as `get_neighbor_value`.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_state_coordinates(state_number):\n    state_x = state_number % LAKE_WIDTH\n    state_y = state_number // LAKE_HEIGHT\n    return state_x, state_y\n\ndef get_max_neighbor(state_number, values):\n    \"\"\"Finds the maximum valued neighbor for a given state.\n    \n    Args:\n        state_number (int): the state to find the max neighbor for\n        state_values (float array): the respective value of each state for\n            each cell of the lake.\n    \n    Returns:\n        max_value (float): the value of the maximum neighbor.\n        policy (int): the action to take to move towards the maximum neighbor.\n    \"\"\"\n    state_x, state_y = get_state_coordinates(state_number)\n    \n    # No policy or best value yet\n    best_policy = -1\n    max_value = -np.inf\n\n    # If the cell has something other than 0, it's a terminal state.\n    if LAKE[state_y, state_x]:\n        return LAKE[state_y, state_x], best_policy\n    \n    for action in ACTION_RANGE:\n        neighbor_value = get_neighbor_value(state_x, state_y, values, action)\n        if neighbor_value > max_value:\n            max_value = neighbor_value\n            best_policy = action\n        \n    return max_value, best_policy","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNow, let's write our value iteration code. \nWe'll write a function that comes out one step of the iteration by checking each state and finding its maximum neighbor. \nThe values will be reshaped so that it's in the form of the lake, but the policy will stay as a list of ints. \nThis way, when Gym returns a state,\nall we need to do is look at the corresponding index in the policy list to tell our agent where to go.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iterate_value(current_values):\n    \"\"\"Finds the future state values for an array of current states.\n    \n    Args:\n        current_values (int array): the value of current states.\n\n    Returns:\n        next_values (int array): The value of states based on future states.\n        next_policies (int array): The recommended action to take in a state.\n    \"\"\"\n    next_values = []\n    next_policies = []\n\n    for state in STATE_RANGE:\n        value, policy = get_max_neighbor(state, current_values)\n        next_values.append(value)\n        next_policies.append(policy)\n    \n    next_values = np.array(next_values).reshape((LAKE_HEIGHT, LAKE_WIDTH))\n    return next_values, next_policies\n\nnext_values, next_policies = iterate_value(current_values)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_values","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array([[ 0,  0,  0,  0],\n       [ 0, -1,  0, -1],\n       [ 0,  0,  0, -1],\n       [-1,  0,  0,  1]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(next_policies).reshape((LAKE_HEIGHT ,LAKE_WIDTH))","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"array([[ 0,  0,  0,  0],\n       [ 0, -1,  0, -1],\n       [ 0,  0,  0, -1],\n       [-1,  0,  0, -1]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_values = DISCOUNT * next_values\ncurrent_values","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"array([[ 0. ,  0. ,  0. ,  0. ],\n       [ 0. , -0.9,  0. , -0.9],\n       [ 0. ,  0. ,  0. , -0.9],\n       [-0.9,  0. ,  0. ,  0.9]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_values, next_policies = iterate_value(current_values)\nprint(\"Value\")\nprint(next_values)\nprint(\"Policy\")\nprint(np.array(next_policies).reshape((4,4)))\ncurrent_values = DISCOUNT * next_values","execution_count":38,"outputs":[{"output_type":"stream","text":"Value\n[[ 0.531441  0.59049   0.6561    0.59049 ]\n [ 0.59049  -1.        0.729    -1.      ]\n [ 0.6561    0.729     0.81     -1.      ]\n [-1.        0.81      0.9       1.      ]]\nPolicy\n[[ 1  2  1  0]\n [ 1 -1  1 -1]\n [ 2  1  1 -1]\n [-1  2  2 -1]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_game(policy):\n    state = env.reset()\n    step = 0\n    done = False\n\n    while not done:\n        action = policy[state]  # This line is new.\n        state, _, done, _ = env.step(action)\n        env.render()\n        step += 1\n        print_state(state, done)\n\nplay_game(next_policies)","execution_count":39,"outputs":[{"output_type":"stream","text":"  (Down)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n15 - Cocoa Time!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Policy Iteration\n#You may have noticed that the first lake was built with the parameter is_slippery=False.\n#This time, we're going to switch it to True.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = gym.make('FrozenLake-v0', is_slippery=True)\nstate = env.reset()\nenv.render()","execution_count":40,"outputs":[{"output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_game(next_policies)","execution_count":41,"outputs":[{"output_type":"stream","text":"  (Down)\nS\u001b[41mF\u001b[0mFF\nFHFH\nFFFH\nHFFG\n1 - Still Alive!\n  (Right)\nSFFF\nF\u001b[41mH\u001b[0mFH\nFFFH\nHFFG\n5 - Game Over!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_future_values(current_values, current_policies):\n    \"\"\"Finds the next set of future values based on the current policy.\"\"\"\n    next_values = []\n\n    for state in STATE_RANGE:\n        current_policy = current_policies[state]\n        state_x, state_y = get_state_coordinates(state)\n\n        # If the cell has something other than 0, it's a terminal state.\n        value = LAKE[state_y, state_x]\n        if not value:\n            value = get_neighbor_value(\n                state_x, state_y, current_values, current_policy)\n        next_values.append(value)\n\n    return np.array(next_values).reshape((LAKE_HEIGHT, LAKE_WIDTH))","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_policy(next_values):\n    \"\"\"Finds the best policy given a value mapping.\"\"\"\n    next_policies = []\n    for state in STATE_RANGE:\n        state_x, state_y = get_state_coordinates(state)\n\n        # No policy or best value yet\n        max_value = -np.inf\n        best_policy = -1\n\n        if not LAKE[state_y, state_x]:\n            for policy in ACTION_RANGE:\n                neighbor_value = get_neighbor_value(\n                    state_x, state_y, next_values, policy)\n                if neighbor_value > max_value:\n                    max_value = neighbor_value\n                    best_policy = policy\n                \n        next_policies.append(best_policy)\n    return next_policies","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iterate_policy(current_values, current_policies):\n    \"\"\"Finds the future state values for an array of current states.\n    \n    Args:\n        current_values (int array): the value of current states.\n        current_policies (int array): a list where each cell is the recommended\n            action for the state matching its index.\n\n    Returns:\n        next_values (int array): The value of states based on future states.\n        next_policies (int array): The recommended action to take in a state.\n    \"\"\"\n    next_values = find_future_values(current_values, current_policies)\n    next_policies = find_best_policy(next_values)\n    return next_values, next_policies","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_locations(state_x, state_y, policy):\n    left = [state_y, state_x-1]\n    down = [state_y+1, state_x]\n    right = [state_y, state_x+1]\n    up = [state_y-1, state_x]\n    directions = [left, down, right, up]\n    num_actions = len(directions)\n\n    gumdrop_right = (policy - 1) % num_actions\n    gumdrop_left = (policy + 1) % num_actions\n    locations = [gumdrop_left, policy, gumdrop_right]\n    return [directions[location] for location in locations]","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_neighbor_value(state_x, state_y, values, policy):\n    \"\"\"Returns the value of a state's neighbor.\n    \n    Args:\n        state_x (int): The state's horizontal position, 0 is the lake's left.\n        state_y (int): The state's vertical position, 0 is the lake's top.\n        values (float array): The current iteration's state values.\n        policy (int): Which action to check the value for.\n        \n    Returns:\n        The corresponding action's value.\n    \"\"\"\n    locations = get_locations(state_x, state_y, policy)\n    location_chance = 1.0 / len(locations)\n    total_value = 0\n\n    for location in locations:\n        check_x = location[1]\n        check_y = location[0]\n\n        is_boulder = check_y < 0 or check_y >= LAKE_HEIGHT \\\n            or check_x < 0 or check_x >= LAKE_WIDTH\n    \n        value = values[state_y, state_x]\n        if not is_boulder:\n            value = values[check_y, check_x]\n        total_value += location_chance * value\n\n    return total_value","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_values = np.zeros_like(LAKE)\npolicies = np.random.choice(ACTION_RANGE, size=STATE_SPACE)\nnp.array(policies).reshape((4,4))","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"array([[2, 3, 3, 3],\n       [1, 1, 1, 0],\n       [1, 2, 2, 2],\n       [3, 0, 1, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_values, policies = iterate_policy(current_values, policies)\nprint(\"Value\")\nprint(next_values)\nprint(\"Policy\")\nprint(np.array(policies).reshape((4,4)))\ncurrent_values = DISCOUNT * next_values","execution_count":54,"outputs":[{"output_type":"stream","text":"Value\n[[ 7.29000e-04  0.00000e+00  0.00000e+00  0.00000e+00]\n [ 6.07500e-03 -1.00000e+00 -2.78778e-01 -1.00000e+00]\n [ 2.80260e-02  9.06660e-02  8.60490e-02 -1.00000e+00]\n [-1.00000e+00  2.35530e-01  5.13579e-01  1.00000e+00]]\nPolicy\n[[ 0  3  3  3]\n [ 0 -1  0 -1]\n [ 3  1  0 -1]\n [-1  2  1 -1]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_game(policies)","execution_count":55,"outputs":[{"output_type":"stream","text":"  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFF\u001b[41mF\u001b[0mH\nHFFG\n10 - Still Alive!\n  (Left)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n14 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n15 - Cocoa Time!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = np.zeros((1, env.action_space.n))\nq_table = np.copy(new_row)\nq_map = {0: 0}\n\ndef print_q(q_table, q_map):\n    print(\"mapping\")\n    print(q_map)\n    print(\"q_table\")\n    print(q_table)\n\nprint_q(q_table, q_map)","execution_count":56,"outputs":[{"output_type":"stream","text":"mapping\n{0: 0}\nq_table\n[[0. 0. 0. 0.]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_action(q_map, q_table, state_row, random_rate):\n    \"\"\"Find max-valued actions and randomly select from them.\"\"\"\n    if random.random() < random_rate:\n        return random.randint(0, ACTION_SPACE-1)\n\n    action_values = q_table[state_row]\n    max_indexes = np.argwhere(action_values == action_values.max())\n    max_indexes = np.squeeze(max_indexes, axis=-1)\n    action = np.random.choice(max_indexes)\n    return action","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_q(q_table, new_state_row, reward, old_value):\n    \"\"\"Returns an updated Q-value based on the Bellman Equation.\"\"\"\n    learning_rate = .1  # Change to be between 0 and 1.\n    future_value = reward + DISCOUNT * np.max(q_table[new_state_row])\n    return old_value + learning_rate * (future_value - old_value)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def play_game(q_table, q_map, random_rate, render=False):\n    state = env.reset()\n    step = 0\n    done = False\n\n    while not done:\n        state_row = q_map[state]\n        action = get_action(q_map, q_table, state_row, random_rate)\n        new_state, _, done, _ = env.step(action)\n\n        #Add new state to table and mapping if it isn't there already.\n        if new_state not in q_map:\n            q_map[new_state] = len(q_table)\n            q_table = np.append(q_table, new_row, axis=0)\n        new_state_row = q_map[new_state]\n\n        reward = -.01  #Encourage exploration.\n        if done:\n            reward = 1 if new_state == 15 else -1\n        current_q = q_table[state_row, action]\n        q_table[state_row, action] = update_q(\n            q_table, new_state_row, reward, current_q)\n\n        step += 1\n        if render:\n            env.render()\n            print_state(new_state, done)\n        state = new_state\n        \n    return q_table, q_map","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run to refresh the q_table.\nrandom_rate = 1\nq_table = np.copy(new_row)\nq_map = {0: 0}","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_table, q_map = play_game(q_table, q_map, random_rate, render=True)\nprint_q(q_table, q_map)","execution_count":61,"outputs":[{"output_type":"stream","text":"  (Down)\nS\u001b[41mF\u001b[0mFF\nFHFH\nFFFH\nHFFG\n1 - Still Alive!\n  (Up)\nS\u001b[41mF\u001b[0mFF\nFHFH\nFFFH\nHFFG\n1 - Still Alive!\n  (Down)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Up)\nS\u001b[41mF\u001b[0mFF\nFHFH\nFFFH\nHFFG\n1 - Still Alive!\n  (Right)\nSFFF\nF\u001b[41mH\u001b[0mFH\nFFFH\nHFFG\n5 - Game Over!\nmapping\n{0: 0, 1: 1, 5: 2}\nq_table\n[[ 0.    -0.001  0.    -0.001]\n [ 0.    -0.001 -0.1   -0.001]\n [ 0.     0.     0.     0.   ]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(1000):\n    q_table, q_map = play_game(q_table, q_map, random_rate)\n    random_rate = random_rate * .99\nprint_q(q_table, q_map)\nrandom_rate","execution_count":62,"outputs":[{"output_type":"stream","text":"mapping\n{0: 0, 1: 1, 5: 2, 4: 3, 2: 4, 6: 5, 10: 6, 7: 7, 3: 8, 8: 9, 12: 10, 9: 11, 14: 12, 13: 13, 11: 14, 15: 15}\nq_table\n[[-0.03262936 -0.14532978 -0.14327662 -0.21812393]\n [-0.42297708 -0.51350574 -0.28919996 -0.06906911]\n [ 0.          0.          0.          0.        ]\n [-0.00625558 -0.2558071  -0.25693556 -0.32065619]\n [-0.2101821  -0.22420294 -0.22987722 -0.09385813]\n [-0.62798693 -0.62307313 -0.22250468 -0.64236568]\n [ 0.24932124 -0.20599133 -0.59662755 -0.38019376]\n [ 0.          0.          0.          0.        ]\n [-0.51916707 -0.48126436 -0.24148579 -0.102249  ]\n [-0.44933207 -0.4603394  -0.50110119  0.06121349]\n [ 0.          0.          0.          0.        ]\n [-0.25816058  0.1987019  -0.16695667 -0.14403122]\n [ 0.027892    0.75367638  0.09378429  0.27825301]\n [-0.27781368 -0.15064756  0.36112681 -0.17677364]\n [ 0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.        ]]\n","name":"stdout"},{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"4.317124741065784e-05"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_table, q_map = play_game(q_table, q_map, 0, render=True)","execution_count":63,"outputs":[{"output_type":"stream","text":"  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n0 - Still Alive!\n  (Left)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n4 - Still Alive!\n  (Left)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n8 - Still Alive!\n  (Up)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n13 - Still Alive!\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n9 - Still Alive!\n  (Down)\nSFFF\nFHFH\nFF\u001b[41mF\u001b[0mH\nHFFG\n10 - Still Alive!\n  (Left)\nSFFF\nFH\u001b[41mF\u001b[0mH\nFFFH\nHFFG\n6 - Still Alive!\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\n7 - Game Over!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}